{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISTAT FOI - Download and Merge FOI Data\n",
    "\n",
    "This notebook downloads FOI (Famiglie e Operatori Istituzionali) data from ISTAT and merges them into a single file.\n",
    "\n",
    "**Data sources:**\n",
    "- 1996-2010: Base 1995=100\n",
    "- 2011-2015: Base 2010=100\n",
    "- 2016-2025: Base 2015=100\n",
    "\n",
    "The notebook downloads the CSV files from ISTAT SDMX API and merges them into `data/istat/FOI_MONTHLY.csv` with columns:\n",
    "- TIME_PERIOD: Date period (YYYY-MM format)\n",
    "- OBS_VALUE: Observation value\n",
    "- BASE: Base year indicator (1995=100, 2010=100, or 2015=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from io import StringIO\n",
    "import urllib3\n",
    "import openpyxl  # For reading Excel files\n",
    "import time\n",
    "\n",
    "# Suppress SSL warnings (equivalent to curl -kL)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download with retry\n",
    "def download_with_retry(url, filepath, headers=None, max_retries=3, retry_delay=5, timeout=180):\n",
    "    \"\"\"\n",
    "    Download a file with retry mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    url : str\n",
    "        URL to download from\n",
    "    filepath : str\n",
    "        Path where to save the file\n",
    "    headers : dict, optional\n",
    "        HTTP headers to use\n",
    "    max_retries : int\n",
    "        Maximum number of retry attempts (default: 3)\n",
    "    retry_delay : int\n",
    "        Delay in seconds between retries (default: 5)\n",
    "    timeout : int\n",
    "        Request timeout in seconds (default: 180)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool : True if download successful, False otherwise\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"  Attempt {attempt}/{max_retries}...\")\n",
    "            response = requests.get(url, headers=headers, verify=False, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save to file\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f\"  Download successful: {filepath}\")\n",
    "            print(f\"  File size: {len(response.content)} bytes\")\n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Attempt {attempt} failed: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                print(f\"  Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"  All {max_retries} attempts failed for {url}\")\n",
    "                return False\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspace\n",
      "Output directory: data/istat\n"
     ]
    }
   ],
   "source": [
    "# Change to project root directory\n",
    "# Find the project root by looking for the 'data' directory\n",
    "current_dir = os.getcwd()\n",
    "while not os.path.exists(os.path.join(current_dir, 'data')):\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    if parent_dir == current_dir:\n",
    "        # Reached filesystem root without finding 'data' directory\n",
    "        raise FileNotFoundError(\"Could not find project root directory (looking for 'data' folder)\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "os.chdir(current_dir)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Define output directory\n",
    "output_dir = 'data/istat'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to download:\n",
      "  foi_2016_2025.csv (Base: 2015=100)\n",
      "  foi_1996_2010_base1995.csv (Base: 1995=100)\n",
      "  foi_2011_2015_base2010.csv (Base: 2010=100)\n"
     ]
    }
   ],
   "source": [
    "# Define the URLs and corresponding base years\n",
    "download_configs = [\n",
    "    {\n",
    "        'url': 'https://esploradati.istat.it/SDMXWS/rest/data/169_745_DF_DCSP_FOI1B2015_1/M.IT.55.4.00?startPeriod=2016-01&endPeriod=2025-12',\n",
    "        'filename': 'foi_2016_2025.csv',\n",
    "        'base': '2015=100'\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://esploradati.istat.it/SDMXWS/rest/data/144_110_DF_DCSP_FOI1_1/M.IT.4.4.00?startPeriod=1996-01&endPeriod=2010-12',\n",
    "        'filename': 'foi_1996_2010_base1995.csv',\n",
    "        'base': '1995=100'\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://esploradati.istat.it/SDMXWS/rest/data/169_15_DF_DCSP_FOI1B2010_1/M.IT.11.4.00?startPeriod=2011-01&endPeriod=2015-12',\n",
    "        'filename': 'foi_2011_2015_base2010.csv',\n",
    "        'base': '2010=100'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Files to download:\")\n",
    "for config in download_configs:\n",
    "    print(f\"  {config['filename']} (Base: {config['base']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Downloading foi_2016_2025.csv...\n",
      "============================================================\n",
      "  Attempt 1/3...\n",
      "  Download successful: data/istat/foi_2016_2025.csv\n",
      "  File size: 9031 bytes\n",
      "\n",
      "============================================================\n",
      "Downloading foi_1996_2010_base1995.csv...\n",
      "============================================================\n",
      "  Attempt 1/3...\n",
      "  Download successful: data/istat/foi_1996_2010_base1995.csv\n",
      "  File size: 12600 bytes\n",
      "\n",
      "============================================================\n",
      "Downloading foi_2011_2015_base2010.csv...\n",
      "============================================================\n",
      "  Attempt 1/3...\n",
      "  Download successful: data/istat/foi_2011_2015_base2010.csv\n",
      "  File size: 4642 bytes\n",
      "\n",
      "\n",
      "Successfully downloaded 3 file(s) out of 3\n"
     ]
    }
   ],
   "source": [
    "# Download all CSV files\n",
    "headers = {\n",
    "    'Accept': 'application/vnd.sdmx.data+csv;version=1.0.0'\n",
    "}\n",
    "\n",
    "downloaded_files = []\n",
    "\n",
    "for config in download_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Downloading {config['filename']}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    filepath = os.path.join(output_dir, config['filename'])\n",
    "    \n",
    "    # Download with retry mechanism\n",
    "    success = download_with_retry(\n",
    "        url=config['url'],\n",
    "        filepath=filepath,\n",
    "        headers=headers,\n",
    "        max_retries=3,\n",
    "        retry_delay=5,\n",
    "        timeout=180\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        downloaded_files.append({\n",
    "            'filepath': filepath,\n",
    "            'base': config['base']\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Failed to download {config['filename']} after all retry attempts\")\n",
    "\n",
    "print(f\"\\n\\nSuccessfully downloaded {len(downloaded_files)} file(s) out of {len(download_configs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process each downloaded file\n",
    "dataframes = []\n",
    "\n",
    "for file_info in downloaded_files:\n",
    "    filepath = file_info['filepath']\n",
    "    base = file_info['base']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {os.path.basename(filepath)} (Base: {base})...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        print(f\"Original columns: {df.columns.tolist()}\")\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Check if TIME_PERIOD and OBS_VALUE columns exist\n",
    "        if 'TIME_PERIOD' not in df.columns:\n",
    "            print(f\"Warning: TIME_PERIOD column not found. Available columns: {df.columns.tolist()}\")\n",
    "            # Try to find a similar column\n",
    "            time_cols = [col for col in df.columns if 'TIME' in col.upper() or 'PERIOD' in col.upper() or 'DATE' in col.upper()]\n",
    "            if time_cols:\n",
    "                df = df.rename(columns={time_cols[0]: 'TIME_PERIOD'})\n",
    "                print(f\"Renamed '{time_cols[0]}' to 'TIME_PERIOD'\")\n",
    "            else:\n",
    "                print(\"Error: Could not find TIME_PERIOD column\")\n",
    "                continue\n",
    "        \n",
    "        if 'OBS_VALUE' not in df.columns:\n",
    "            print(f\"Warning: OBS_VALUE column not found. Available columns: {df.columns.tolist()}\")\n",
    "            # Try to find a similar column\n",
    "            obs_cols = [col for col in df.columns if 'OBS' in col.upper() or 'VALUE' in col.upper()]\n",
    "            if obs_cols:\n",
    "                df = df.rename(columns={obs_cols[0]: 'OBS_VALUE'})\n",
    "                print(f\"Renamed '{obs_cols[0]}' to 'OBS_VALUE'\")\n",
    "            else:\n",
    "                print(\"Error: Could not find OBS_VALUE column\")\n",
    "                continue\n",
    "        \n",
    "        # Extract only TIME_PERIOD and OBS_VALUE\n",
    "        df_extracted = df[['TIME_PERIOD', 'OBS_VALUE']].copy()\n",
    "        \n",
    "        # Add BASE column\n",
    "        df_extracted['BASE'] = base\n",
    "        \n",
    "        # Convert OBS_VALUE to numeric\n",
    "        df_extracted['OBS_VALUE'] = pd.to_numeric(df_extracted['OBS_VALUE'], errors='coerce')\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        df_extracted = df_extracted.dropna()\n",
    "        \n",
    "        print(f\"\\nProcessed {len(df_extracted)} rows\")\n",
    "        print(f\"Time period range: {df_extracted['TIME_PERIOD'].min()} to {df_extracted['TIME_PERIOD'].max()}\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df_extracted.head(10))\n",
    "        \n",
    "        dataframes.append(df_extracted)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\\nSuccessfully processed {len(dataframes)} file(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "if len(dataframes) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Merging all dataframes...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    df_merged = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Sort by TIME_PERIOD\n",
    "    df_merged = df_merged.sort_values(by='TIME_PERIOD').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "    print(f\"Columns: {df_merged.columns.tolist()}\")\n",
    "    print(f\"\\nTime period range: {df_merged['TIME_PERIOD'].min()} to {df_merged['TIME_PERIOD'].max()}\")\n",
    "    print(f\"\\nBase year distribution:\")\n",
    "    print(df_merged['BASE'].value_counts())\n",
    "    \n",
    "    print(f\"\\nFirst 20 rows:\")\n",
    "    print(df_merged.head(20))\n",
    "    \n",
    "    print(f\"\\nLast 20 rows:\")\n",
    "    print(df_merged.tail(20))\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df_merged.duplicated(subset=['TIME_PERIOD'], keep=False)\n",
    "    if duplicates.any():\n",
    "        print(f\"\\nWarning: Found {duplicates.sum()} duplicate TIME_PERIOD entries:\")\n",
    "        print(df_merged[duplicates].sort_values(by='TIME_PERIOD'))\n",
    "    else:\n",
    "        print(f\"\\nNo duplicate TIME_PERIOD entries found\")\n",
    "    \n",
    "else:\n",
    "    print(\"No dataframes to merge!\")\n",
    "    df_merged = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged file\n",
    "if df_merged is not None:\n",
    "    output_file = os.path.join(output_dir, 'FOI_MONTHLY.csv')\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_merged.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"File saved successfully: {output_file}\")\n",
    "    print(f\"Dimensions: {len(df_merged)} rows, {len(df_merged.columns)} columns\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"No data to save!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Splicing Coefficients\n",
    "\n",
    "Download the Excel file with splicing coefficients needed to convert all data to base 1995=100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download splicing coefficients Excel file\n",
    "splicing_url = \"https://esploradati.istat.it/databrowser/DWL/Prezzi/DCSP_FOI1B2015/DCSP_Splicing_coefficient_TB4.xls\"\n",
    "splicing_filename = \"DCSP_Splicing_coefficient_TB4_FOI.xls\"\n",
    "splicing_filepath = os.path.join(output_dir, splicing_filename)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Downloading splicing coefficients file...\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"URL: {splicing_url}\")\n",
    "print(f\"Output: {splicing_filepath}\")\n",
    "\n",
    "# Download with retry mechanism\n",
    "success = download_with_retry(\n",
    "    url=splicing_url,\n",
    "    filepath=splicing_filepath,\n",
    "    headers=None,\n",
    "    max_retries=3,\n",
    "    retry_delay=5,\n",
    "    timeout=180\n",
    ")\n",
    "\n",
    "if not success:\n",
    "    raise RuntimeError(f\"Failed to download splicing coefficients file after all retry attempts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Splicing Coefficients\n",
    "\n",
    "Read the \"EN\" tab from the Excel file and extract coefficients for ECOICOP=00, level=Gen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file, specifically the \"EN\" tab\n",
    "# First, analyze the structure to find where headers start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Analyzing Excel file structure...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "try:\n",
    "    # Read first 20 rows without header to analyze structure\n",
    "    # Try with openpyxl first, then fall back to xlrd for .xls files\n",
    "    df_raw = None\n",
    "    try:\n",
    "        df_raw = pd.read_excel(splicing_filepath, sheet_name='EN', header=None, nrows=20, engine='openpyxl')\n",
    "    except Exception as e1:\n",
    "        print(f\"Trying with openpyxl failed: {e1}\")\n",
    "        try:\n",
    "            df_raw = pd.read_excel(splicing_filepath, sheet_name='EN', header=None, nrows=20, engine='xlrd')\n",
    "        except Exception as e2:\n",
    "            print(f\"Trying with xlrd failed: {e2}\")\n",
    "            # Try without specifying engine\n",
    "            df_raw = pd.read_excel(splicing_filepath, sheet_name='EN', header=None, nrows=20)\n",
    "    \n",
    "    print(f\"Raw data shape: {df_raw.shape}\")\n",
    "    print(f\"\\nFirst 20 rows (raw):\")\n",
    "    print(df_raw.to_string())\n",
    "    \n",
    "    # Find the header row by looking for column names like ECOICOP, Level, or coefficient names\n",
    "    header_row = None\n",
    "    for idx in range(min(20, len(df_raw))):\n",
    "        row = df_raw.iloc[idx]\n",
    "        row_str = ' '.join([str(val) for val in row.values if pd.notna(val)]).upper()\n",
    "        # Look for keywords that indicate this is the header row\n",
    "        # Check specifically for ECOICOP (not just any mention of numbers)\n",
    "        if 'ECOICOP' in row_str or 'ECOICOP' in str(row.values):\n",
    "            header_row = idx\n",
    "            print(f\"\\nFound potential header row at index {idx}:\")\n",
    "            print(row)\n",
    "            break\n",
    "    \n",
    "    if header_row is None:\n",
    "        # If we can't find a clear header row, try row 2 (based on ISTAT structure)\n",
    "        print(\"\\nWarning: Could not identify header row, trying row 2\")\n",
    "        header_row = 2\n",
    "    \n",
    "    print(f\"\\nUsing row {header_row} as header\")\n",
    "    \n",
    "    # Now read the full file with the correct header row\n",
    "    try:\n",
    "        df_splicing = pd.read_excel(splicing_filepath, sheet_name='EN', header=header_row, engine='openpyxl')\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            df_splicing = pd.read_excel(splicing_filepath, sheet_name='EN', header=header_row, engine='xlrd')\n",
    "        except Exception as e2:\n",
    "            df_splicing = pd.read_excel(splicing_filepath, sheet_name='EN', header=header_row)\n",
    "    \n",
    "    print(f\"\\nExcel file shape: {df_splicing.shape}\")\n",
    "    print(f\"Columns: {df_splicing.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_splicing.head(20))\n",
    "    \n",
    "    # Find the row with ECOICOP=00 and level=Gen.\n",
    "    print(f\"\\nSearching for ECOICOP=00 and level=Gen...\")\n",
    "    \n",
    "    # Try to find the relevant columns\n",
    "    ecoicop_col = None\n",
    "    level_col = None\n",
    "    coef_1995_2010_col = None\n",
    "    coef_2010_2015_col = None\n",
    "    \n",
    "    print(f\"\\nSearching for columns in dataframe with {len(df_splicing.columns)} columns:\")\n",
    "    for idx, col in enumerate(df_splicing.columns):\n",
    "        col_upper = str(col).upper()\n",
    "        col_str = str(col)\n",
    "        col_lower = str(col).lower()\n",
    "        print(f\"  Column {idx}: '{col}'\")\n",
    "        \n",
    "        if 'ECOICOP' in col_upper or 'COICOP' in col_upper:\n",
    "            ecoicop_col = col\n",
    "            print(f\"    -> Identified as ECOICOP column\")\n",
    "        if 'LEVEL' in col_upper or 'LIV' in col_upper or 'level' in col_lower:\n",
    "            level_col = col\n",
    "            print(f\"    -> Identified as level column\")\n",
    "        if ('1995' in col_str and '2010' in col_str) or ('1995' in col_upper and '2010' in col_upper):\n",
    "            coef_1995_2010_col = col\n",
    "            print(f\"    -> Identified as coefficient 1995->2010 column\")\n",
    "        if ('2010' in col_str and '2015' in col_str) or ('2010' in col_upper and '2015' in col_upper):\n",
    "            coef_2010_2015_col = col\n",
    "            print(f\"    -> Identified as coefficient 2010->2015 column\")\n",
    "    \n",
    "    print(f\"\\nIdentified columns:\")\n",
    "    print(f\"  ECOICOP column: {ecoicop_col}\")\n",
    "    print(f\"  Level column: {level_col}\")\n",
    "    print(f\"  Coefficient 1995->2010 column: {coef_1995_2010_col}\")\n",
    "    print(f\"  Coefficient 2010->2015 column: {coef_2010_2015_col}\")\n",
    "    \n",
    "    # Filter for ECOICOP=00 and level=Gen.\n",
    "    # Try different variations: \"00\", \"0\", \"Gen\", \"Gen.\", \"General\"\n",
    "    df_filtered = None\n",
    "    \n",
    "    if ecoicop_col and level_col:\n",
    "        print(f\"\\nTrying to filter with ECOICOP column '{ecoicop_col}' and level column '{level_col}'...\")\n",
    "        print(f\"Unique ECOICOP values: {df_splicing[ecoicop_col].astype(str).str.strip().unique()}\")\n",
    "        print(f\"Unique level values: {df_splicing[level_col].astype(str).str.strip().unique()}\")\n",
    "        \n",
    "        # Try exact match first with '00'\n",
    "        mask = (\n",
    "            (df_splicing[ecoicop_col].astype(str).str.strip() == '00') &\n",
    "            (df_splicing[level_col].astype(str).str.strip().str.contains('Gen', na=False, case=False))\n",
    "        )\n",
    "        if mask.any():\n",
    "            df_filtered = df_splicing[mask]\n",
    "            print(f\"Found with exact match '00'\")\n",
    "        else:\n",
    "            # Try with '0' (single zero)\n",
    "            mask = (\n",
    "                (df_splicing[ecoicop_col].astype(str).str.strip() == '0') &\n",
    "                (df_splicing[level_col].astype(str).str.strip().str.contains('Gen', na=False, case=False))\n",
    "            )\n",
    "            if mask.any():\n",
    "                df_filtered = df_splicing[mask]\n",
    "                print(f\"Found with exact match '0'\")\n",
    "            else:\n",
    "                # Try with contains '00'\n",
    "                mask = (\n",
    "                    (df_splicing[ecoicop_col].astype(str).str.contains('00', na=False, regex=False)) &\n",
    "                    (df_splicing[level_col].astype(str).str.contains('Gen', na=False, case=False, regex=False))\n",
    "                )\n",
    "                if mask.any():\n",
    "                    df_filtered = df_splicing[mask]\n",
    "                    print(f\"Found with contains '00'\")\n",
    "    \n",
    "    # If still not found, search in all columns\n",
    "    if df_filtered is None or len(df_filtered) == 0:\n",
    "        print(\"\\nTrying alternative search method in all columns...\")\n",
    "        # Look for rows where any column contains '00' or '0' and any column contains 'Gen'\n",
    "        mask_00 = pd.Series([False] * len(df_splicing))\n",
    "        mask_gen = pd.Series([False] * len(df_splicing))\n",
    "        \n",
    "        for col in df_splicing.columns:\n",
    "            col_str = df_splicing[col].astype(str)\n",
    "            # Try both '00' and '0'\n",
    "            mask_00 = mask_00 | (col_str.str.strip() == '00') | (col_str.str.strip() == '0')\n",
    "            mask_gen = mask_gen | col_str.str.contains('Gen', na=False, case=False, regex=False)\n",
    "        \n",
    "        mask = mask_00 & mask_gen\n",
    "        if mask.any():\n",
    "            df_filtered = df_splicing[mask]\n",
    "            print(f\"Found with alternative search method\")\n",
    "        else:\n",
    "            # Last resort: show all rows to debug\n",
    "            print(\"\\nCould not find ECOICOP=00 and level=Gen. Showing all rows:\")\n",
    "            print(df_splicing.to_string())\n",
    "            df_filtered = pd.DataFrame()  # Empty dataframe\n",
    "    \n",
    "    print(f\"\\nFiltered rows: {len(df_filtered)}\")\n",
    "    if len(df_filtered) > 0:\n",
    "        print(f\"\\nFiltered data:\")\n",
    "        print(df_filtered.to_string())\n",
    "        \n",
    "        # Extract the splicing coefficients\n",
    "        print(f\"\\nAll columns in filtered data:\")\n",
    "        for col in df_filtered.columns:\n",
    "            print(f\"  {col}: {df_filtered[col].values}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: No rows found matching ECOICOP=00 and level=Gen\")\n",
    "        print(\"Showing first 10 rows of the dataframe for debugging:\")\n",
    "        print(df_splicing.head(10).to_string())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the splicing coefficients\n",
    "# The coefficients we need are:\n",
    "# 1. Coefficient from base 1995 to base 2010\n",
    "# 2. Coefficient from base 2010 to base 2015\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Extracting splicing coefficients...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "coefficient_1995_to_2010 = None\n",
    "coefficient_2010_to_2015 = None\n",
    "\n",
    "if len(df_filtered) > 0:\n",
    "    # Get the first matching row\n",
    "    row = df_filtered.iloc[0]\n",
    "    \n",
    "    print(f\"\\nRow data:\")\n",
    "    for col in df_filtered.columns:\n",
    "        print(f\"  {col}: {row[col]} (type: {type(row[col])})\")\n",
    "    \n",
    "    # First, try to use the column names we identified earlier\n",
    "    if coef_1995_2010_col and coef_1995_2010_col in df_filtered.columns:\n",
    "        try:\n",
    "            coefficient_1995_to_2010 = pd.to_numeric(row[coef_1995_2010_col], errors='coerce')\n",
    "            if pd.notna(coefficient_1995_to_2010):\n",
    "                print(f\"\\nFound coefficient 1995->2010 in column '{coef_1995_2010_col}': {coefficient_1995_to_2010}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting coefficient 1995->2010 from column '{coef_1995_2010_col}': {e}\")\n",
    "    \n",
    "    if coef_2010_2015_col and coef_2010_2015_col in df_filtered.columns:\n",
    "        try:\n",
    "            coefficient_2010_to_2015 = pd.to_numeric(row[coef_2010_2015_col], errors='coerce')\n",
    "            if pd.notna(coefficient_2010_to_2015):\n",
    "                print(f\"Found coefficient 2010->2015 in column '{coef_2010_2015_col}': {coefficient_2010_to_2015}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting coefficient 2010->2015 from column '{coef_2010_2015_col}': {e}\")\n",
    "    \n",
    "    # If we still don't have the coefficients, look for numeric values in all columns\n",
    "    if coefficient_1995_to_2010 is None or coefficient_2010_to_2015 is None:\n",
    "        print(f\"\\nSearching for numeric values in all columns...\")\n",
    "        numeric_values = []\n",
    "        for col in df_filtered.columns:\n",
    "            try:\n",
    "                val = pd.to_numeric(row[col], errors='coerce')\n",
    "                if pd.notna(val) and 0.1 < val < 10.0:\n",
    "                    col_name = str(col).upper()\n",
    "                    col_str = str(col)\n",
    "                    print(f\"  Found numeric value in column '{col}': {val}\")\n",
    "                    # Try to identify which coefficient this is based on column name\n",
    "                    if ('1995' in col_name and '2010' in col_name) or ('1995' in col_str and '2010' in col_str):\n",
    "                        if coefficient_1995_to_2010 is None:\n",
    "                            coefficient_1995_to_2010 = val\n",
    "                            print(f\"    -> Identified as coefficient 1995->2010: {val}\")\n",
    "                    elif ('2010' in col_name and '2015' in col_name) or ('2010' in col_str and '2015' in col_str):\n",
    "                        if coefficient_2010_to_2015 is None:\n",
    "                            coefficient_2010_to_2015 = val\n",
    "                            print(f\"    -> Identified as coefficient 2010->2015: {val}\")\n",
    "                    else:\n",
    "                        # Store for fallback\n",
    "                        numeric_values.append((col, val))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        # If we still don't have both, use the numeric values found\n",
    "        # Based on the Excel structure, the coefficients should be in the last two numeric columns\n",
    "        # The expected values are: 1.385 for 1995->2010 and 1.074 for 2010->2015\n",
    "        if coefficient_1995_to_2010 is None:\n",
    "            if len(numeric_values) >= 2:\n",
    "                # Use the first numeric value as 1995->2010 (should be around 1.385)\n",
    "                coefficient_1995_to_2010 = numeric_values[0][1]\n",
    "                print(f\"\\nUsing first numeric value as coefficient 1995->2010: {coefficient_1995_to_2010} (from column '{numeric_values[0][0]}')\")\n",
    "            elif len(numeric_values) == 1:\n",
    "                coefficient_1995_to_2010 = numeric_values[0][1]\n",
    "                print(f\"\\nUsing only numeric value as coefficient 1995->2010: {coefficient_1995_to_2010} (from column '{numeric_values[0][0]}')\")\n",
    "        \n",
    "        if coefficient_2010_to_2015 is None:\n",
    "            if len(numeric_values) >= 2:\n",
    "                # Use the second numeric value as 2010->2015 (should be around 1.074)\n",
    "                coefficient_2010_to_2015 = numeric_values[1][1]\n",
    "                print(f\"Using second numeric value as coefficient 2010->2015: {coefficient_2010_to_2015} (from column '{numeric_values[1][0]}')\")\n",
    "            elif len(numeric_values) == 1 and coefficient_1995_to_2010 != numeric_values[0][1]:\n",
    "                # If we only have one numeric value and it's different from the first coefficient, use it\n",
    "                coefficient_2010_to_2015 = numeric_values[0][1]\n",
    "                print(f\"Using numeric value as coefficient 2010->2015: {coefficient_2010_to_2015} (from column '{numeric_values[0][0]}')\")\n",
    "        \n",
    "        # If still missing, try to get from column indices (assuming they are in the last two columns)\n",
    "        if (coefficient_1995_to_2010 is None or coefficient_2010_to_2015 is None) and len(df_filtered.columns) >= 2:\n",
    "            print(f\"\\nTrying to extract from column indices...\")\n",
    "            # Get the last two columns that might contain the coefficients\n",
    "            last_cols = list(df_filtered.columns)[-2:]\n",
    "            for idx, col in enumerate(last_cols):\n",
    "                try:\n",
    "                    val = pd.to_numeric(row[col], errors='coerce')\n",
    "                    if pd.notna(val) and 0.1 < val < 10.0:\n",
    "                        if idx == 0 and coefficient_1995_to_2010 is None:\n",
    "                            coefficient_1995_to_2010 = val\n",
    "                            print(f\"Extracted coefficient 1995->2010 from last-2 column '{col}': {val}\")\n",
    "                        elif idx == 1 and coefficient_2010_to_2015 is None:\n",
    "                            coefficient_2010_to_2015 = val\n",
    "                            print(f\"Extracted coefficient 2010->2015 from last-1 column '{col}': {val}\")\n",
    "                except:\n",
    "                    pass\n",
    "else:\n",
    "    print(\"ERROR: No filtered rows found. Cannot extract coefficients.\")\n",
    "    raise ValueError(\"Could not find row with ECOICOP=00 and level=Gen in Excel file\")\n",
    "\n",
    "print(f\"\\nFinal coefficients:\")\n",
    "print(f\"  1995 -> 2010: {coefficient_1995_to_2010}\")\n",
    "print(f\"  2010 -> 2015: {coefficient_2010_to_2015}\")\n",
    "\n",
    "if coefficient_1995_to_2010 is None or coefficient_2010_to_2015 is None:\n",
    "    raise ValueError(f\"Could not extract both splicing coefficients from Excel file. Found: 1995->2010={coefficient_1995_to_2010}, 2010->2015={coefficient_2010_to_2015}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spliced Dataset (Base 1995=100)\n",
    "\n",
    "Convert all data to base 1995=100 using the splicing coefficients:\n",
    "- 1996-2010: Already in base 1995, no conversion needed\n",
    "- 2011-2015: Convert from base 2010 to base 1995\n",
    "- 2016-2025: Convert from base 2015 to base 1995\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spliced dataset\n",
    "if df_merged is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Creating spliced dataset (base 1995=100)...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create a copy of the merged dataframe\n",
    "    df_spliced = df_merged.copy()\n",
    "    \n",
    "    # Add a new column for the spliced value\n",
    "    df_spliced['OBS_VALUE_SPLICED'] = df_spliced['OBS_VALUE'].copy()\n",
    "    \n",
    "    # Apply conversions based on BASE and TIME_PERIOD\n",
    "    for idx, row in df_spliced.iterrows():\n",
    "        base = row['BASE']\n",
    "        time_period = row['TIME_PERIOD']\n",
    "        obs_value = row['OBS_VALUE']\n",
    "        \n",
    "        # Extract year from TIME_PERIOD (format: YYYY-MM)\n",
    "        year = int(time_period.split('-')[0])\n",
    "        \n",
    "        if base == '1995=100':\n",
    "            # Segment 1996-2010: Already in base 1995, no conversion\n",
    "            df_spliced.at[idx, 'OBS_VALUE_SPLICED'] = obs_value\n",
    "            \n",
    "        elif base == '2010=100':\n",
    "            # Segment 2011-2015: Convert from base 2010 to base 1995\n",
    "            # FOIt95 = FOIt10 × coefficient from base 1995 to base 2010\n",
    "            df_spliced.at[idx, 'OBS_VALUE_SPLICED'] = obs_value * coefficient_1995_to_2010\n",
    "            \n",
    "        elif base == '2015=100':\n",
    "            # Segment 2016-2025: Convert from base 2015 to base 1995\n",
    "            # FOIt95 = FOIt15 × coefficient 1995->2010 × coefficient 2010->2015\n",
    "            df_spliced.at[idx, 'OBS_VALUE_SPLICED'] = obs_value * coefficient_1995_to_2010 * coefficient_2010_to_2015\n",
    "    \n",
    "    # Create final dataset with only TIME_PERIOD and OBS_VALUE_SPLICED\n",
    "    df_final = pd.DataFrame({\n",
    "        'TIME_PERIOD': df_spliced['TIME_PERIOD'],\n",
    "        'OBS_VALUE': df_spliced['OBS_VALUE_SPLICED']\n",
    "    })\n",
    "    \n",
    "    # Sort by TIME_PERIOD\n",
    "    df_final = df_final.sort_values(by='TIME_PERIOD').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nSpliced dataset shape: {df_final.shape}\")\n",
    "    print(f\"Time period range: {df_final['TIME_PERIOD'].min()} to {df_final['TIME_PERIOD'].max()}\")\n",
    "    print(f\"\\nFirst 20 rows:\")\n",
    "    print(df_final.head(20))\n",
    "    print(f\"\\nLast 20 rows:\")\n",
    "    print(df_final.tail(20))\n",
    "    \n",
    "    # Show conversion examples\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Conversion examples:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Example from each segment\n",
    "    example_1995 = df_spliced[df_spliced['BASE'] == '1995=100'].iloc[0]\n",
    "    example_2010 = df_spliced[df_spliced['BASE'] == '2010=100'].iloc[0]\n",
    "    example_2015 = df_spliced[df_spliced['BASE'] == '2015=100'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nBase 1995 (no conversion):\")\n",
    "    print(f\"  TIME_PERIOD: {example_1995['TIME_PERIOD']}\")\n",
    "    print(f\"  Original: {example_1995['OBS_VALUE']:.4f}\")\n",
    "    print(f\"  Spliced: {example_1995['OBS_VALUE_SPLICED']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBase 2010 -> 1995:\")\n",
    "    print(f\"  TIME_PERIOD: {example_2010['TIME_PERIOD']}\")\n",
    "    print(f\"  Original: {example_2010['OBS_VALUE']:.4f}\")\n",
    "    print(f\"  Coefficient (1995->2010): {coefficient_1995_to_2010:.6f}\")\n",
    "    print(f\"  Conversion factor: {coefficient_1995_to_2010:.6f}\")\n",
    "    print(f\"  Spliced: {example_2010['OBS_VALUE_SPLICED']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBase 2015 -> 1995:\")\n",
    "    print(f\"  TIME_PERIOD: {example_2015['TIME_PERIOD']}\")\n",
    "    print(f\"  Original: {example_2015['OBS_VALUE']:.4f}\")\n",
    "    print(f\"  Coefficient (1995->2010): {coefficient_1995_to_2010:.6f}\")\n",
    "    print(f\"  Coefficient (2010->2015): {coefficient_2010_to_2015:.6f}\")\n",
    "    print(f\"  Conversion factor: {coefficient_1995_to_2010 * coefficient_2010_to_2015:.6f}\")\n",
    "    print(f\"  Spliced: {example_2015['OBS_VALUE_SPLICED']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No merged data available for splicing!\")\n",
    "    df_final = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot FOI Data\n",
    "\n",
    "Create plots to visualize the original data (with different bases) and the spliced data (all in base 1995=100).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# Create plots\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Creating plots...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Read the data files\n",
    "df_original = pd.read_csv(os.path.join(output_dir, 'FOI_MONTHLY.csv'))\n",
    "df_spliced = pd.read_csv(os.path.join(output_dir, 'FOI_MONTHLY_SPLICED.csv'))\n",
    "\n",
    "# Convert TIME_PERIOD to datetime for plotting\n",
    "df_original['DATE'] = pd.to_datetime(df_original['TIME_PERIOD'], format='%Y-%m')\n",
    "df_spliced['DATE'] = pd.to_datetime(df_spliced['TIME_PERIOD'], format='%Y-%m')\n",
    "\n",
    "# Plot 1: Original data with different bases\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot original data by base\n",
    "ax1 = axes[0]\n",
    "for base in df_original['BASE'].unique():\n",
    "    df_base = df_original[df_original['BASE'] == base]\n",
    "    ax1.plot(df_base['DATE'], df_base['OBS_VALUE'], \n",
    "             label=f'Base {base}', linewidth=1.5, alpha=0.8, marker='o', markersize=3)\n",
    "\n",
    "ax1.set_title('ISTAT FOI - Original Data (Different Bases)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('FOI Index', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=10, framealpha=0.9)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax1.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Plot spliced data (all in base 1995=100)\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df_spliced['DATE'], df_spliced['OBS_VALUE'], \n",
    "         label='Spliced (Base 1995=100)', linewidth=2, color='#2E86AB', alpha=0.8, marker='o', markersize=3)\n",
    "\n",
    "ax2.set_title('ISTAT FOI - Spliced Data (All in Base 1995=100)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('FOI Index (Base 1995=100)', fontsize=12)\n",
    "ax2.legend(loc='best', fontsize=10, framealpha=0.9)\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax2.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plot_file = os.path.join(output_dir, 'FOI_MONTHLY_plots.png')\n",
    "plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved: {plot_file}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Comparative plot (original vs spliced)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot original data by base with different colors\n",
    "colors = {'1995=100': '#1f77b4', '2010=100': '#ff7f0e', '2015=100': '#2ca02c'}\n",
    "for base in df_original['BASE'].unique():\n",
    "    df_base = df_original[df_original['BASE'] == base]\n",
    "    ax.plot(df_base['DATE'], df_base['OBS_VALUE'], \n",
    "            label=f'Original (Base {base})', linewidth=1.5, alpha=0.6, \n",
    "            color=colors.get(base, 'gray'), linestyle='--', marker='o', markersize=2)\n",
    "\n",
    "# Plot spliced data\n",
    "ax.plot(df_spliced['DATE'], df_spliced['OBS_VALUE'], \n",
    "        label='Spliced (Base 1995=100)', linewidth=2.5, color='#d62728', alpha=0.9, marker='o', markersize=3)\n",
    "\n",
    "ax.set_title('ISTAT FOI - Original vs Spliced Data Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=13)\n",
    "ax.set_ylabel('FOI Index', fontsize=13)\n",
    "ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the comparative plot\n",
    "plot_file_comparative = os.path.join(output_dir, 'FOI_MONTHLY_comparative_plot.png')\n",
    "plt.savefig(plot_file_comparative, dpi=150, bbox_inches='tight')\n",
    "print(f\"Comparative plot saved: {plot_file_comparative}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Plots created successfully!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the spliced file\n",
    "if df_final is not None:\n",
    "    output_file_spliced = os.path.join(output_dir, 'FOI_MONTHLY_SPLICED.csv')\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_final.to_csv(output_file_spliced, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"File saved successfully: {output_file_spliced}\")\n",
    "    print(f\"Dimensions: {len(df_final)} rows, {len(df_final.columns)} columns\")\n",
    "    print(f\"All data converted to base 1995=100\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"No data to save!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
