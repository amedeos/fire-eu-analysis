{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCI Indices - Download and Fill Missing Days\n",
    "\n",
    "This notebook downloads MSCI index data from the GitHub repository and fills missing days using backward fill (bfill).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project root directory\n",
    "# Find the project root by looking for the 'data' directory\n",
    "current_dir = os.getcwd()\n",
    "while not os.path.exists(os.path.join(current_dir, 'data')):\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    if parent_dir == current_dir:\n",
    "        # Reached filesystem root without finding 'data' directory\n",
    "        raise FileNotFoundError(\"Could not find project root directory (looking for 'data' folder)\")\n",
    "    current_dir = parent_dir\n",
    "\n",
    "os.chdir(current_dir)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub repository base URL\n",
    "github_base_url = \"https://raw.githubusercontent.com/amedeos/Stock-Indexes-Historical-Data/main\"\n",
    "\n",
    "# Define the files to download\n",
    "files_to_download = {\n",
    "    'ACWI': 'DAILY/NET/EUR/ALL-COUNTRY-DM-EM/Region/NONE/NONE/STANDARD-LARGE-MID-CAP/ACWI.csv',\n",
    "    'WORLD': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Region/NONE/NONE/STANDARD-LARGE-MID-CAP/WORLD.csv',\n",
    "    'ITALY': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/ITALY.csv',\n",
    "    'EUROPE': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Region/NONE/NONE/STANDARD-LARGE-MID-CAP/EUROPE.csv',\n",
    "    'EMU': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Region/NONE/NONE/STANDARD-LARGE-MID-CAP/EMU.csv',\n",
    "    'GERMANY': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/GERMANY.csv',\n",
    "    'FRANCE': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/FRANCE.csv',\n",
    "    'SPAIN': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/SPAIN.csv',\n",
    "    'UK': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/UK.csv',\n",
    "    'NETHERLANDS': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/NETHERLANDS.csv',\n",
    "    'SWEDEN': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/SWEDEN.csv',\n",
    "    'DENMARK': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/DENMARK.csv',\n",
    "    'NORWAY': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/NORWAY.csv',\n",
    "    'FINLAND': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/FINLAND.csv',\n",
    "    'SWITZERLAND': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/SWITZERLAND.csv',\n",
    "    'AUSTRIA': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/AUSTRIA.csv',\n",
    "    'BELGIUM': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/BELGIUM.csv',\n",
    "    'PORTUGAL': 'DAILY/NET/EUR/DEVELOPED-MARKETS-DM/Country/NONE/NONE/STANDARD-LARGE-MID-CAP/PORTUGAL.csv'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = 'data/msci'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"\\nFiles to download:\")\n",
    "for name, path in files_to_download.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process each file\n",
    "for index_name, file_path in files_to_download.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {index_name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Construct the full URL\n",
    "    file_url = f\"{github_base_url}/{file_path}\"\n",
    "    print(f\"Downloading from: {file_url}\")\n",
    "    \n",
    "    # Download the file\n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Download successful (size: {len(response.content)} bytes)\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Read CSV into pandas\n",
    "    df = pd.read_csv(StringIO(response.text))\n",
    "    print(f\"\\nOriginal data shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Identify the date column (common names: Date, DATE, date, TIME_PERIOD, etc.)\n",
    "    date_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['date', 'time_period', 'time', 'timestamp']:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col is None:\n",
    "        print(f\"Warning: Could not identify date column. Assuming first column is date.\")\n",
    "        date_col = df.columns[0]\n",
    "    \n",
    "    print(f\"\\nUsing '{date_col}' as date column\")\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    \n",
    "    # Check for missing dates\n",
    "    print(f\"\\nDate range: {df[date_col].min()} to {df[date_col].max()}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    \n",
    "    # Create a complete date range (daily)\n",
    "    date_range = pd.date_range(start=df[date_col].min(), end=df[date_col].max(), freq='D')\n",
    "    print(f\"Expected daily rows: {len(date_range)}\")\n",
    "    print(f\"Missing days: {len(date_range) - len(df)}\")\n",
    "    \n",
    "    # Set date as index\n",
    "    df_indexed = df.set_index(date_col)\n",
    "    \n",
    "    # Reindex to include all days\n",
    "    df_complete = df_indexed.reindex(date_range)\n",
    "    \n",
    "    # Apply backward fill (bfill) to fill missing days\n",
    "    df_filled = df_complete.bfill()\n",
    "    \n",
    "    # Reset index to have date as a column again\n",
    "    df_filled = df_filled.reset_index()\n",
    "    df_filled = df_filled.rename(columns={'index': date_col})\n",
    "    \n",
    "    print(f\"\\nAfter filling missing days:\")\n",
    "    print(f\"Total rows: {len(df_filled)}\")\n",
    "    print(f\"Missing values per column:\")\n",
    "    print(df_filled.isnull().sum())\n",
    "    \n",
    "    # Save to output directory\n",
    "    output_file = os.path.join(output_dir, f\"{index_name}.csv\")\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved to: {output_file}\")\n",
    "    print(f\"Final shape: {df_filled.shape}\")\n",
    "    \n",
    "    # Identify the value column (first numeric column that is not the date)\n",
    "    value_col = None\n",
    "    for col in df_filled.columns:\n",
    "        if col != date_col and pd.api.types.is_numeric_dtype(df_filled[col]):\n",
    "            value_col = col\n",
    "            break\n",
    "    \n",
    "    if value_col is None:\n",
    "        print(f\"Warning: Could not identify value column for plotting.\")\n",
    "    else:\n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df_filled[date_col], df_filled[value_col], linewidth=1.5)\n",
    "        plt.title(f'MSCI {index_name} Index - Historical Performance', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel(f'{value_col}', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Format x-axis dates\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Save plot\n",
    "        plot_file = os.path.join(output_dir, f\"{index_name}_plot.png\")\n",
    "        plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {plot_file}\")\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
